import requests

url = 'https://realpython.github.io/fake-jobs/'  # Replace with the actual URL of the e-commerce website
response = requests.get(url)

if response.status_code == 200:
    page_content = response.content
else:
    print('Failed to retrieve the page')

from bs4 import BeautifulSoup

soup = BeautifulSoup(page_content, 'html.parser')

# Example: Find product details
products = soup.find_all('div', class_='product-item')
product_list = []

for product in products:
    name = product.find('h2', class_='product-title').text
    price = product.find('span', class_='product-price').text
    link = product.find('a', class_='product-link')['href']
    product_list.append({
        'Name': name,
        'Price': price,
        'Link': link
    })

import pandas as pd

df = pd.DataFrame(product_list)
df.to_csv('products.csv', index=False)

base_url = 'https://realpython.github.io/fake-jobs'
product_list = []

for page in range(1, 11):  # Adjust the range as needed
    url = base_url + str(page)
    response = requests.get(url)
    if response.status_code == 200:
        page_content = response.content
        soup = BeautifulSoup(page_content, 'html.parser')
        products = soup.find_all('div', class_='product-item')

        for product in products:
            name = product.find('h2', class_='product-title').text
            price = product.find('span', class_='product-price').text
            link = product.find('a', class_='product-link')['href']
            product_list.append({
                'Name': name,
                'Price': price,
                'Link': link
            })
    else:
        print(f'Failed to retrieve page {page}')

# Save to CSV
df = pd.DataFrame(product_list)
df.to_csv('products.csv', index=False)
